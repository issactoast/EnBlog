---
title: 'The Coefficient of Linear Regression in R  - gmp package, Cholesky and QR decomposition'
author: Issac Lee
date: '2019-10-12'
slug: linearmodelcholqr
categories:
  - R
  - machine learning
  - linear model
tags:
  - gmp
  - cholesky decomposition
  - QR decomposition
header:
  caption: 'Image credit: [UIowaStat](https://stat.uiowa.edu/)'
  image: 'headers/longley-wide.png'
  preview: yes
---

This article is also one of my portfolio articles, but I should notice that this article is written based on a wornderful course that I took at the University of Iowa; [the intensive computing course](http://homepage.divms.uiowa.edu/~luke/classes/STAT7400/).

The following are the `R` packages used in this post.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(tibble)
```

```{r message=FALSE, warning=FALSE}
library(datasets)
library(magrittr)
library(microbenchmark)
library(gmp)
```

## Normal equation for the beta

Today, I want to talk little bit about the process of getting the coefficients of linear model in `R`. Among many methods, our main focus for today is related to Cholesky decomposition and QR decomposition.

The reason why we are discussing about the matrix decomposition methods for the linear regression is that the coefficients of the regression is the solution of the matrix equation called Normal equation.

Let us we have the following $n$ observation of vector $y$ and the data matrix $X$. If assume that the vector $y$ follows a linear model with noise vector $\mathbf{e}$, then the situation can be written as follows;

$$
\begin{eqnarray*}
\mathbf{y}_{n\times1} & = & X_{n\times p}\boldsymbol{\beta}_{p\times1}+\mathbf{e}_{n\times1}\\
 & = & \left[\begin{array}{ccccc}
\mathbf{1} & \mathbf{x}_{1} & \mathbf{x}_{2} & ... & \mathbf{x}_{p-1}\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{p-1}
\end{array}\right]+\mathbf{e}\\
 & = & \beta_{0}\mathbf{1}+\beta_{1}\mathbf{x}_{1}+\beta_{2}\mathbf{x}_{2}+...+\beta_{p-1}\mathbf{x}_{p-1}+\mathbf{e}
\end{eqnarray*}
$$

Note that $\mathbf{1}$, $\mathbf{e}$ and $\mathbf{x}_{i}$, $i=1,...,p-1$ are $n \times 1$ vectors.

Under this assumption, the coefficients the linear model, $\beta$, which minimizes the residual sum of squares (RSS), can be obtained by solving the following Normal eqaution;

$$
X^{T}X\mathbf{\beta}=X^{T}\mathbf{y}
$$

For the details about the Normal equation can be found at [wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#Derivation_directly_in_terms_of_matrices).

## `longley` data set

<div align="center">
![The longley data in the paper](https://raw.githubusercontent.com/issactoast/EnBlog/master/static/img/longley.png){width=560}
</div>

The Longley data set belongs to the appendix of the paper written by J. W. Longley (1967); An appraisal of least-squares programs from the point of view of the user. The data set is minimal toy data which consists of 7 explanatory variables. This data is famous for the high correlation among the variables when we apply the linear regression of the `Employed` variable on the rest of the variables. The data set is contained in the `datasets` package, so let us load the data set.

```{r eval=FALSE}
library(datasets)
longley
```

```{r results = "asis"}
kable(longley, format = "html") %>% 
  kable_styling() %>% 
  scroll_box(width = "100%", height = "300px")
```

We can see that the `longley` data set in `R` was scaled compare to the original data.


## Calculating the accurate coefficients using `gmp` package

The `gmp` package uses fractions for its calculation instead of using decimal points. This package allows us to achieve more accurate values for the coefficient since it can avoid the truncation of floating numbers. By using solve function, let us calculate the accurate coefficients of the linear regression of the `Employed` variable on the rest of the variables in the `longley` data set.

```{r }
y <- as.vector(longley[,7])
X <- as.matrix(cbind(constant = 1, longley[,-7]))

# install.packages("gmp")
# library(gmp)

# X, y rationals
r_X <- as.bigq(round(1000 * X)) / as.bigq(1000)
r_y <- as.bigq(round(1000 * y)) / as.bigq(1000)
head(r_X)

# coefficients
cef_exact <- as.double(solve(t(r_X) %*% r_X,
                             (t(r_X) %*% r_y)))
cef_exact
```


## Cholesky decomposition

For a matrix $A$, if the matrix $A$ satisfies $x^T \mathbf{A} x > 0$ for every $x$, then the matrix $A$ is a positive definite. 

The Cholesky decomposition can be applied to positive definite matrices. If a matrix $A$ is a positive definite matrix then there exists a lower triangular matrix, $L$, such that

$$
\mathbf{A} = \mathbf{L} \mathbf{L}^T
$$

Thus, if our data matrix is a full rank, in other words, if the matrix $X^TX$ is a positive definite, then the Normal equation can be written as follows;

$$
\begin{align*}
\mathbf{X}^{T}\mathbf{X}\beta & =\mathbf{X}^{T}y\\
\Rightarrow\mathbf{L}\mathbf{L}^{T}\beta & =\mathbf{X}^{T}y\\
\tag{1}\Rightarrow\mathbf{L}\left(\mathbf{L}^{T}\beta\right) & =\mathbf{X}^{T}y
\end{align*}
$$

따라서 위의 식에서 $\beta$ 벡터는 다음의 두 단계를 통해서 구할 수 있다.


<a href="https://theissaclee.com/project/telematicsproject/" target="_self">Return to the telematics project page</a>


